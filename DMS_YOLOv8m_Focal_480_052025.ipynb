{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO2xEvf2H2kMrTBooVflOFo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VesalAhsani/Driver-behavior-detection/blob/main/DMS_YOLOv8m_Focal_480_052025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv8-m Classification @ 480px on Colab\n",
        "\n",
        "Make sure you’ve set your Runtime ▶️ “Change runtime type” ▶️ GPU."
      ],
      "metadata": {
        "id": "Hwyuvjd1W0KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1️⃣ Install dependencies\n",
        "# Colab comes with torch+cuda preinstalled, but we upgrade ultralytics.\n",
        "!pip install --upgrade ultralytics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGAHcBnuWz8N",
        "outputId": "97f078ba-861f-4c8b-cf0e-0609c2ddccda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2️⃣ Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Qitw8GWz0K",
        "outputId": "69dc0a2b-5569-49bb-9210-3ab7b39c0588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3️⃣ Paths & Unzip Dataset\n",
        "import os, zipfile, glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Adjust these to match where you put your ZIP in Drive:\n",
        "ZIP_PATH     = \"/content/drive/MyDrive/Split_mixed_newaugmented_480.zip\"\n",
        "EXTRACT_DIR  = \"/content/dataset480\"\n",
        "SPLIT_DIR    = os.path.join(EXTRACT_DIR, \"Split_480\")  # must contain train/ & val/\n",
        "RESULTS_DIR  = \"/content/drive/MyDrive/yolov8m_cls_480_results\"\n",
        "\n",
        "# 3.1 Unzip if needed\n",
        "if not os.path.isdir(SPLIT_DIR):\n",
        "    print(\"📦 Unzipping dataset…\")\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
        "        z.extractall(EXTRACT_DIR)\n",
        "    print(\"✅ Unzipped to\", EXTRACT_DIR)\n",
        "\n",
        "# 3.2 Verify\n",
        "assert os.path.isdir(os.path.join(SPLIT_DIR, \"train\")), \"❌ train/ missing!\"\n",
        "assert os.path.isdir(os.path.join(SPLIT_DIR, \"val\"  )), \"❌ val/   missing!\"\n",
        "\n",
        "train_images = glob.glob(os.path.join(SPLIT_DIR, \"train\", \"*\", \"*.jpg\"))\n",
        "val_images   = glob.glob(os.path.join(SPLIT_DIR, \"val\",   \"*\", \"*.jpg\"))\n",
        "print(f\"🧾 Found {len(train_images):,} train and {len(val_images):,} val images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NASvHCt2WzxW",
        "outputId": "b05841da-7348-4d78-96eb-d578dc3d02e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Unzipping dataset…\n",
            "✅ Unzipped to /content/dataset480\n",
            "🧾 Found 156,504 train and 17,276 val images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4️⃣ Compute α (class imbalance) for focal loss\n",
        "import torch\n",
        "\n",
        "# your class counts\n",
        "class_counts = {\n",
        "  \"Control_Panel\":11700, \"Drinking\":11700, \"Eating\":2689,   \"Makeup\":11700,\n",
        "  \"Normal\":12196,        \"Phone_Call_(Left)\":11700, \"Phone_Call_(Right)\":11700,\n",
        "  \"Reaching_Behind\":11700,\"Sleep\":12232,  \"Smoking\":12376,\n",
        "  \"Talk_to_passengers\":11700,\"Text_(Left)\":11700, \"Text_(Right)\":11700,\n",
        "  \"Yawning\":11700,\n",
        "}\n",
        "\n",
        "max_n = max(class_counts.values())\n",
        "alpha = torch.tensor([max_n / class_counts[c] for c in class_counts],\n",
        "                     dtype=torch.float32).cuda()\n",
        "print(\"✅ α weights:\", alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp81JzuHWzu5",
        "outputId": "2094cc4e-c9a8-4984-f26c-9d41d851d540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ α weights: tensor([1.0578, 1.0578, 4.6025, 1.0578, 1.0148, 1.0578, 1.0578, 1.0578, 1.0118,\n",
            "        1.0000, 1.0578, 1.0578, 1.0578, 1.0578], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5️⃣ Define FocalLoss & Train YOLOv8-m-cls\n",
        "import torch.nn as nn\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ─── Focal Loss ──────────────────────────\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma, self.alpha, self.reduction = gamma, alpha, reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        logp = torch.nn.functional.log_softmax(inputs, dim=-1)\n",
        "        p    = torch.exp(logp)\n",
        "        t    = targets.long()\n",
        "        pt   = p.gather(-1, t.unsqueeze(-1)).squeeze(-1)\n",
        "        focal = (1 - pt) ** self.gamma\n",
        "        loss = -focal * logp.gather(-1, t.unsqueeze(-1)).squeeze(-1)\n",
        "        if self.alpha is not None:\n",
        "            loss = loss * self.alpha[t]\n",
        "        return loss.mean() if self.reduction=='mean' else loss.sum()\n",
        "\n",
        "# ─── Load & override loss ─────────────────\n",
        "print(\"🚀 Loading YOLOv8m-cls…\")\n",
        "# model = YOLO(\"yolov8m-cls.pt\")             # pretrained backbone\n",
        "model = YOLO('/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/last.pt')\n",
        "model.loss = FocalLoss(gamma=2.0, alpha=alpha)\n",
        "\n",
        "# ─── Train ───────────────────────────────\n",
        "print(\"📚 Training…\")\n",
        "model.train(\n",
        "    data=SPLIT_DIR,          # train/ & val/ subfolders\n",
        "    epochs=50,\n",
        "    batch=32,\n",
        "    imgsz=480,\n",
        "    project=RESULTS_DIR,\n",
        "    name=\"yolov8m_cls_480_focal\",\n",
        "    pretrained=False,\n",
        "    lr0=5e-4,\n",
        "    optimizer=\"Adam\",\n",
        "    resume=True,\n",
        ")\n",
        "print(\"✅ Training resumed!\")\n",
        "\n",
        "\n",
        "# ─── Validate & copy best.pt ────────────\n",
        "print(\"📊 Validating…\")\n",
        "model.val()\n",
        "\n",
        "src = os.path.join(model.trainer.save_dir, \"weights/best.pt\")\n",
        "dst = os.path.join(RESULTS_DIR, \"yolov8m_cls_480_focal\", \"weights/best.pt\")\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "os.system(f\"cp {src} {dst}\")\n",
        "print(\"✅ Best weights saved to:\", dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrkC8DNzWzrS",
        "outputId": "1dd357ab-30ef-46ba-a23a-6bd6e55b8dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "🚀 Loading YOLOv8m-cls…\n",
            "📚 Training…\n",
            "Ultralytics 8.3.127 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/dataset480/Split_480, degrees=0.0, deterministic=True, device=cuda:0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=480, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8m_cls_480_focal, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/yolov8m_cls_480_results, rect=False, resume=/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/last.pt, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset480/Split_480/train... found 156504 images in 14 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/dataset480/Split_480/val... found 17276 images in 14 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
            "  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n",
            "  9                  -1  1   1003534  ultralytics.nn.modules.head.Classify         [768, 14]                     \n",
            "YOLOv8m-cls summary: 80 layers, 15,790,270 parameters, 15,790,270 gradients, 41.9 GFLOPs\n",
            "Transferred 230/230 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 96.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1563.8±554.5 MB/s, size: 43.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset480/Split_480/train... 156504 images, 0 corrupt: 100%|██████████| 156504/156504 [00:43<00:00, 3585.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset480/Split_480/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1236.0±444.2 MB/s, size: 47.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset480/Split_480/val... 17276 images, 0 corrupt: 100%|██████████| 17276/17276 [00:05<00:00, 3430.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset480/Split_480/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.0005, momentum=0.937) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n",
            "Resuming training /content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/last.pt from epoch 49 to 50 total epochs\n",
            "Image sizes 480 train, 480 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50      5.31G    0.08113         24        480: 100%|██████████| 4891/4891 [16:35<00:00,  4.92it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 270/270 [00:38<00:00,  7.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.995          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50      6.49G    0.07889         24        480: 100%|██████████| 4891/4891 [16:34<00:00,  4.92it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 270/270 [00:38<00:00,  7.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.995          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2 epochs completed in 0.576 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/last.pt, 31.7MB\n",
            "Optimizer stripped from /content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/best.pt, 31.7MB\n",
            "\n",
            "Validating /content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/best.pt...\n",
            "Ultralytics 8.3.127 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLOv8m-cls summary (fused): 42 layers, 15,780,590 parameters, 0 gradients, 41.6 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset480/Split_480/train... found 156504 images in 14 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/dataset480/Split_480/val... found 17276 images in 14 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc:   2%|▏         | 6/270 [00:00<00:38,  6.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc:   4%|▍         | 11/270 [00:01<00:34,  7.41it/s]\n",
            "100%|██████████| 755k/755k [00:00<00:00, 23.5MB/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 270/270 [00:36<00:00,  7.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.995          1\n",
            "Speed: 0.3ms preprocess, 1.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal\u001b[0m\n",
            "✅ Training resumed!\n",
            "📊 Validating…\n",
            "Ultralytics 8.3.127 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLOv8m-cls summary (fused): 42 layers, 15,780,590 parameters, 0 gradients, 41.6 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/dataset480/Split_480/train... found 156504 images in 14 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/dataset480/Split_480/val... found 17276 images in 14 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1952.5±676.3 MB/s, size: 47.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset480/Split_480/val... 17276 images, 0 corrupt: 100%|██████████| 17276/17276 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 540/540 [00:59<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.995          1\n",
            "Speed: 0.3ms preprocess, 3.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal2\u001b[0m\n",
            "✅ Best weights saved to: /content/drive/MyDrive/yolov8m_cls_480_results/yolov8m_cls_480_focal/weights/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import runtime\n",
        "\n",
        "# Wait for training to complete before disconnecting\n",
        "print(\"Training completed! Releasing GPU resources...\")\n",
        "time.sleep(20)  # Give some time for final processing\n",
        "\n",
        "# Automatically disconnect the Colab session\n",
        "runtime.unassign()\n",
        "os._exit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dboKANCY1Md",
        "outputId": "26a655a7-5063-4a87-83f3-141f360160aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed! Releasing GPU resources...\n"
          ]
        }
      ]
    }
  ]
}